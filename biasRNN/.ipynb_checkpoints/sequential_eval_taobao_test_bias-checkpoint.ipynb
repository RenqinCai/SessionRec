{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "use time to cut sequences\n",
    "command \n",
    "python main_time.py --data_folder ../Data/xing/ --train_data train_item.pickle --valid_data test_item.pickle --test_data test_item.pickle --data_name xing --embedding_dim 300 --hidden_size 300 --lr 0.005\n",
    "\"\"\"\n",
    "import argparse\n",
    "import torch\n",
    "# import lib\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from loss import *\n",
    "from network import *\n",
    "from optimizer import *\n",
    "from trainer import *\n",
    "from torch.utils import data\n",
    "import pickle\n",
    "import sys\n",
    "from dataset_time import *\n",
    "# from data_time import *\n",
    "from logger import *\n",
    "import collections\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../PyTorch_GBW_LM')\n",
    "sys.path.insert(0, '../../PyTorch_GBW_LM/log_uniform')\n",
    "\n",
    "from sampledSoftmax import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--hidden_size', default=50, type=int)\n",
    "parser.add_argument('--num_layers', default=1, type=int)\n",
    "parser.add_argument('--batch_size', default=100, type=int)\n",
    "parser.add_argument('--dropout_input', default=0, type=float)\n",
    "parser.add_argument('--dropout_hidden', default=.2, type=float)\n",
    "\n",
    "# parse the optimizer arguments\n",
    "parser.add_argument('--optimizer_type', default='Adagrad', type=str)\n",
    "parser.add_argument('--final_act', default='tanh', type=str)\n",
    "parser.add_argument('--lr', default=.05, type=float)\n",
    "parser.add_argument('--weight_decay', default=0.0, type=float)\n",
    "parser.add_argument('--momentum', default=0.1, type=float)\n",
    "parser.add_argument('--eps', default=1e-6, type=float)\n",
    "\n",
    "parser.add_argument(\"-seed\", type=int, default=7,\n",
    "                     help=\"Seed for random initialization\")\n",
    "parser.add_argument(\"-sigma\", type=float, default=None,\n",
    "                     help=\"init weight -1: range [-sigma, sigma], -2: range [0, sigma]\")\n",
    "parser.add_argument(\"--embedding_dim\", type=int, default=-1,\n",
    "                     help=\"using embedding\")\n",
    "# parse the loss type\n",
    "parser.add_argument('--loss_type', default='TOP1', type=str)\n",
    "# parser.add_argument('--loss_type', default='BPR', type=str)\n",
    "parser.add_argument('--topk', default=5, type=int)\n",
    "# etc\n",
    "parser.add_argument('--bptt', default=1, type=int)\n",
    "parser.add_argument('--test_observed', default=5, type=int)\n",
    "parser.add_argument('--window_size', default=30, type=int)\n",
    "parser.add_argument('--warm_start', default=5, type=int)\n",
    "\n",
    "parser.add_argument('--n_epochs', default=20, type=int)\n",
    "parser.add_argument('--time_sort', default=False, type=bool)\n",
    "parser.add_argument('--save_dir', default='models', type=str)\n",
    "parser.add_argument('--data_folder', default='../Data/movielen/1m/', type=str)\n",
    "parser.add_argument('--data_action', default='item.pickle', type=str)\n",
    "parser.add_argument('--data_cate', default='cate.pickle', type=str)\n",
    "parser.add_argument('--data_time', default='time.pickle', type=str)\n",
    "parser.add_argument(\"--is_eval\", action='store_true')\n",
    "parser.add_argument('--load_model', default=None,  type=str)\n",
    "parser.add_argument('--checkpoint_dir', type=str, default='checkpoint')\n",
    "parser.add_argument('--data_name', default=None, type=str)\n",
    "parser.add_argument('--shared_embedding', default=None, type=int)\n",
    "parser.add_argument('--patience', default=1000)\n",
    "parser.add_argument('--negative_num', default=1000, type=int)\n",
    "parser.add_argument('--valid_start_time', default=0, type=int)\n",
    "parser.add_argument('--test_start_time', default=0, type=int)\n",
    "parser.add_argument('--model_name', default=\"samplePaddingSessionRNN\", type=str)\n",
    "\n",
    "# Get the arguments\n",
    "args = parser.parse_args([])\n",
    "args.cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=0 python eval_main_time.py --data_folder ../Data/tmall/100k_unknown_cate/ \n",
    "# --data_action item_time.pickle --data_cate cate_time.pickle --data_time time_time.pickle \n",
    "# --data_name taobao --embedding_dim 300 --hidden_size 300 --lr 0.001 --window_size 20 \n",
    "# --test_observed 5 --n_epochs 100 --shared_embedding 1 --batch_size 300 \n",
    "# --optimizer_type Adam --loss_type 'XE' --valid_start_time 1512172800 --test_start_time 1512259200 \n",
    "# --negative_num 10000 --topk 20 --checkpoint_dir \"../log/samplePaddingSessionRNN/checkpoint/01022149\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_folder = \"../../../Data/tmall/100k_unknown_cate/\"\n",
    "args.data_action = \"item_time.pickle\"\n",
    "args.data_cate = \"cate_time.pickle\"\n",
    "args.data_time = \"time_time.pickle\"\n",
    "args.data_name = \"taobao\"\n",
    "args.embedding_dim = 256\n",
    "args.hidden_size = 256\n",
    "args.lr = 0.001\n",
    "args.window_size = 20\n",
    "args.test_observed = 5\n",
    "args.n_epochs = 100\n",
    "args.shared_embedding = 1\n",
    "args.batch_size = 256\n",
    "args.optimizer_type = \"Adam\"\n",
    "args.loss_type = \"XE\"\n",
    "args.valid_start_time = 1512172800\n",
    "args.test_start_time = 1512259200\n",
    "args.negative_num = 10000\n",
    "args.topk = 20\n",
    "args.checkpoint_dir = \"../../log/samplePaddingSessionRNN/checkpoint/01031151\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(7)\n",
    "random.seed(args.seed)\n",
    "\n",
    "if args.cuda:\n",
    "    print(\"gpu\")\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "else:\n",
    "    print(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1512172800: 12/02/2017 @ 12:00am (UTC)\n",
    "### 1512187200: 12/02/2017 @ 4:00am (UTC)\n",
    "## 1512201600: 12/02/2017 @ 8:00am (UTC)\n",
    "### 1512216000: 12/02/2017 @ 12:00pm (UTC)\n",
    "### 1512230400: 12/02/2017 @ 4:00pm (UTC)\n",
    "### 1512244800: 12/02/2017 @ 8:00pm (UTC)\n",
    "# 1512259200: 12/03/2017 @ 12:00am (UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_checkpoint_dir(log):\n",
    "    print(\"PARAMETER\" + \"-\"*10)\n",
    "    now = datetime.datetime.now()\n",
    "    S = '{:02d}{:02d}{:02d}{:02d}'.format(now.month, now.day, now.hour, now.minute)\n",
    "    checkpoint_dir = \"../log/\"+args.model_name+\"/\"+args.checkpoint_dir\n",
    "    args.checkpoint_dir = checkpoint_dir\n",
    "    save_dir = os.path.join(args.checkpoint_dir, S)\n",
    "\n",
    "    if not os.path.exists(\"../log\"):\n",
    "        os.mkdir(\"../log\")\n",
    "    \n",
    "    if not os.path.exists(\"../log/\"+args.model_name):\n",
    "        os.mkdir(\"../log/\"+args.model_name)\n",
    "\n",
    "    if not os.path.exists(args.checkpoint_dir):\n",
    "        os.mkdir(args.checkpoint_dir)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    args.checkpoint_dir = save_dir\n",
    "    \n",
    "    with open(os.path.join(args.checkpoint_dir, 'parameter.txt'), 'w') as f:\n",
    "        for attr, value in sorted(args.__dict__.items()):\n",
    "            msg = \"{}={}\".format(attr.upper(), value)\n",
    "            log.addOutput2IO(msg)\n",
    "            f.write(\"{}={}\\n\".format(attr.upper(), value))\n",
    "\n",
    "    msg = \"---------\" + \"-\"*10\n",
    "    log.addOutput2IO(msg)\n",
    "\n",
    "def load_args(model_path):\n",
    "    model_file = os.path.join(model_path, \"model_best.pt\")\n",
    "    print(\"args file load\", model_file)\n",
    "    check_point = torch.load(model_file, map_location=torch.device('cpu'))\n",
    "    args = check_point['args']\n",
    "\n",
    "def load_model(network, model_path):\n",
    "    print(\"reload model\")\n",
    "    model_file = os.path.join(model_path, \"model_best.pt\")\n",
    "    print(\"model file\", model_file)\n",
    "    check_point = torch.load(model_file)\n",
    "\n",
    "    network.load_state_dict(check_point['model'])\n",
    "\n",
    "def count_parameters(model):\n",
    "    parameter_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"parameter_num\", parameter_num) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args file load ../../log/samplePaddingSessionRNN/checkpoint/01031151/model_best.pt\n",
      "device cuda\n",
      "**********train load**********\n",
      "action seq num 51275\n",
      "time seq num 51275\n",
      "loading item map\n",
      "loading item map\n",
      "observed_threshold 5 20\n",
      "loading data\n",
      "valid_start_time 1512172800\n",
      "test start time 1512259200\n",
      "seq num for training 2738883\n",
      "seq num of actions for training 2738883\n",
      "seq num for testing 430797\n",
      "seq num of actions for testing 430797\n",
      "load data duration  0:00:09.721161\n",
      "++++++++++valid load++++++++++\n",
      "item num 68008\n",
      "seq num 2738883\n",
      "batch size 256\n",
      "batch_num 10698\n",
      "seq num 430797\n",
      "batch size 256\n",
      "batch_num 1682\n",
      "seq num 430797\n",
      "batch size 256\n",
      "batch_num 1682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/sr3hd/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reload model\n",
      "model file ../../log/samplePaddingSessionRNN/checkpoint/01031151/model_best.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = args.checkpoint_dir\n",
    "load_args(model_path)\n",
    "\n",
    "BPTT = args.bptt\n",
    "\n",
    "device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "print(\"device\", device)\n",
    "\n",
    "if args.embedding_dim == -1:\n",
    "    raise AssertionError()\n",
    "\n",
    "data_name = args.data_name\n",
    "\n",
    "print(\"*\"*10+\"train load\"+\"*\"*10)\n",
    "\n",
    "observed_threshold = args.test_observed\n",
    "\n",
    "data_action = args.data_folder+args.data_action\n",
    "data_cate = args.data_folder+args.data_cate\n",
    "data_time = args.data_folder+args.data_time\n",
    "\n",
    "valid_start_time = args.valid_start_time\n",
    "test_start_time = args.test_start_time\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "data_obj = MYDATA(data_action, data_cate, data_time, valid_start_time, test_start_time, observed_threshold, args.window_size)\n",
    "et = datetime.datetime.now()\n",
    "print(\"load data duration \", et-st)\n",
    "\n",
    "train_data = data_obj.train_dataset\n",
    "valid_data = data_obj.test_dataset\n",
    "test_data = data_obj.test_dataset\n",
    "\n",
    "print(\"+\"*10+\"valid load\"+\"+\"*10)\n",
    "\n",
    "input_size = data_obj.items()\n",
    "output_size = input_size\n",
    "\n",
    "negative_num = args.negative_num\n",
    "\n",
    "train_data_loader = MYDATALOADER(train_data, args.batch_size)\n",
    "valid_data_loader = MYDATALOADER(valid_data, args.batch_size)\n",
    "test_data_loader = MYDATALOADER(valid_data, args.batch_size)\n",
    "\n",
    "ss = SampledSoftmax(output_size, negative_num, args.embedding_dim, None)\n",
    "\n",
    "network = NETWORK(input_size, ss, args, device)\n",
    "load_model(network, model_path)\n",
    "\n",
    "### eval\n",
    "loss_function = LossFunction(device, loss_type=args.loss_type)\n",
    "\n",
    "topk = args.topk\n",
    "eval = Evaluation(None, network, loss_function, device, topk, args.warm_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get time id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1512172800: 12/02/2017 @ 12:00am (UTC)\n",
    "### 1512187200: 12/02/2017 @ 4:00am (UTC)\n",
    "## 1512201600: 12/02/2017 @ 8:00am (UTC)\n",
    "### 1512216000: 12/02/2017 @ 12:00pm (UTC)\n",
    "### 1512230400: 12/02/2017 @ 4:00pm (UTC)\n",
    "### 1512244800: 12/02/2017 @ 8:00pm (UTC)\n",
    "# 1512259200: 12/03/2017 @ 12:00am (UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeid(time):\n",
    "    time_threshold_list = [1512172800, 1512187200, 1512201600, 1512216000, 1512230400, 1512244800, 1512259200]\n",
    "#     print(\"time threshold num\", len(time_threshold_list))\n",
    "    \n",
    "    timeid = 0\n",
    "    \n",
    "    if time <= time_threshold_list[1]:\n",
    "        timeid = 1\n",
    "    elif time <= time_threshold_list[2]:\n",
    "        timeid = 2\n",
    "    elif time <= time_threshold_list[3]:\n",
    "        timeid = 3\n",
    "    elif time <= time_threshold_list[4]:\n",
    "        timeid = 4\n",
    "    elif time <= time_threshold_list[5]:\n",
    "        timeid = 5\n",
    "    else:\n",
    "        timeid = 6\n",
    "        \n",
    "    return timeid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_item_freq_dict = {}\n",
    "# time_bucket_recall_dict = {}\n",
    "# time_bucket_mrr_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_eval(eval_data):\n",
    "    network.eval()\n",
    "\n",
    "    losses = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    weights = []\n",
    "\n",
    "    dataloader = eval_data\n",
    "    topk = args.topk\n",
    "    \n",
    "    ### time: item: [recall]\n",
    "    time_item_recall_dict = {}\n",
    "    time_item_mrr_dict = {}\n",
    "\n",
    "    \n",
    "### t_y_batch: time of y action\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_test_num = []\n",
    "\n",
    "        for x_short_action_batch, mask_short_action_batch, pad_x_short_actionNum_batch, \\\n",
    "        y_action_batch, y_action_idx_batch, t_y_batch in dataloader:\n",
    "            \n",
    "            x_short_action_batch = x_short_action_batch.to(device)\n",
    "            mask_short_action_batch = mask_short_action_batch.to(device)\n",
    "            y_action_batch = y_action_batch.to(device)\n",
    "\n",
    "            # warm_start_mask = (y_action_idx_batch>=self.warm_start)\n",
    "\n",
    "            output_batch = network(x_short_action_batch, mask_short_action_batch, pad_x_short_actionNum_batch)\n",
    "\n",
    "            sampled_logit_batch, sampled_target_batch = network.m_ss(output_batch, y_action_batch, \\\n",
    "                                                        None, None, None, None, None, None, \"full\")\n",
    "\n",
    "            loss_batch = loss_function(sampled_logit_batch, sampled_target_batch)\n",
    "            losses.append(loss_batch.item())\n",
    "\n",
    "            _, preds = torch.topk(sampled_logit_batch, topk, -1)\n",
    "            preds = preds.cpu()\n",
    "            targets = sampled_target_batch.cpu()\n",
    "\n",
    "            expand_targets = targets.view(-1, 1).expand_as(preds)\n",
    "            hits = (preds == expand_targets)\n",
    "\n",
    "            for i, hit in enumerate(hits):\n",
    "                target_i = targets[i]\n",
    "                itemid_i = target_i.item()\n",
    "                time_i = t_y_batch[i].item()\n",
    "                timeid = get_timeid(time_i)\n",
    "                \n",
    "                if timeid not in time_item_recall_dict:\n",
    "                    time_item_recall_dict[timeid] = {}\n",
    "                    time_item_mrr_dict[timeid] = {}\n",
    "                if itemid_i not in time_item_recall_dict[timeid]:\n",
    "                    time_item_recall_dict[timeid][itemid_i] = []\n",
    "                    time_item_mrr_dict[timeid][itemid_i] = []\n",
    "                \n",
    "                rank = hit.nonzero()\n",
    "                \n",
    "                if len(rank) == 1:\n",
    "                    time_item_recall_dict[timeid][itemid_i].append(1.0)\n",
    "                    rank = rank[0]+1.0\n",
    "                    rank = torch.reciprocal(rank.float())\n",
    "                    time_item_mrr_dict[timeid][itemid_i].append(rank.item())\n",
    "                else:\n",
    "                    time_item_recall_dict[timeid][itemid_i].append(0.0)\n",
    "                    time_item_mrr_dict[timeid][itemid_i].append(0.0)\n",
    "                \n",
    "            total_test_num.append(y_action_batch.view(-1).size(0))\n",
    "            \n",
    "    return time_item_recall_dict, time_item_mrr_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_item_freq(eval_data, time_item_freq_dict):\n",
    "    network.eval()\n",
    "\n",
    "    losses = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    weights = []\n",
    "\n",
    "    dataloader = eval_data\n",
    "    topk = args.topk\n",
    "    \n",
    "    ### time: item: freq\n",
    "    \n",
    "    ### t_action_batch: time of y action\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_test_num = []\n",
    "\n",
    "        for x_short_action_batch, mask_short_action_batch, pad_x_short_actionNum_batch, \\\n",
    "        y_action_batch, y_action_idx_batch, t_y_batch in dataloader:\n",
    "            batch_size = y_action_batch.size(0)\n",
    "            for seq_index in range(batch_size):\n",
    "                y_i = y_action_batch[seq_index]\n",
    "                t_i = t_y_batch[seq_index]\n",
    "                item_i = y_i.item()\n",
    "                time_id = get_timeid(t_i.item())\n",
    "                \n",
    "                if time_id not in time_item_freq_dict:\n",
    "                    time_item_freq_dict[time_id] = {}\n",
    "                if item_i not in time_item_freq_dict[time_id]:\n",
    "                    time_item_freq_dict[time_id][item_i] = 0.0\n",
    "                time_item_freq_dict[time_id][item_i] += 1.0\n",
    "        \n",
    "        for time in time_item_freq_dict.keys():\n",
    "            if time > 1:\n",
    "                for t in range(1, time):\n",
    "                    for item in time_item_freq_dict[t]:\n",
    "                        if item not in time_item_freq_dict[time]:\n",
    "                            time_item_freq_dict[time][item] = 0.0\n",
    "                        time_item_freq_dict[time][item] += time_item_freq_dict[t][item]\n",
    "#     return time_item_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_freq(eval_data, item_freq_dict):\n",
    "    network.eval()\n",
    "\n",
    "    losses = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    weights = []\n",
    "\n",
    "    dataloader = eval_data\n",
    "    topk = args.topk\n",
    "    \n",
    "    ### time: item: freq\n",
    "    \n",
    "    ### t_action_batch: time of y action\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_test_num = []\n",
    "\n",
    "        for x_short_action_batch, mask_short_action_batch, pad_x_short_actionNum_batch, \\\n",
    "        y_action_batch, y_action_idx_batch, t_y_batch in dataloader:\n",
    "            batch_size = y_action_batch.size(0)\n",
    "            for seq_index in range(batch_size):\n",
    "                y_i = y_action_batch[seq_index]\n",
    "                t_i = t_y_batch[seq_index]\n",
    "                item_i = y_i.item()\n",
    "                \n",
    "                if item_i not in item_freq_dict:\n",
    "                    item_freq_dict[item_i] = 0.0\n",
    "                item_freq_dict[item_i] += 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_freq(time_item_freq_dict, item_freq_dict):\n",
    "    for time in time_item_freq_dict:\n",
    "        item_freq_dict_time = time_item_freq_dict[time]\n",
    "        \n",
    "        for item in item_freq_dict:\n",
    "            if item not in item_freq_dict_time:\n",
    "                item_freq_dict_time[item] = 0.0\n",
    "            item_freq_dict_time[item] += item_freq_dict[item]\n",
    "    \n",
    "    time_item_freq_dict[0] = {}\n",
    "    for item in item_freq_dict:\n",
    "        time_item_freq_dict[0][item] = item_freq_dict[item]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_freq_dict = {}\n",
    "# get_item_freq(train_data_loader, train_item_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling\n"
     ]
    }
   ],
   "source": [
    "time_item_freq_dict = {}\n",
    "get_time_item_freq(test_data_loader, time_item_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_train_freq(time_item_freq_dict, train_item_freq_dict) # Create time 0 where there are no items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bucket4item(time_item_freq_dict):\n",
    "#     item_freq_dict = dict(Counter(data.m_y_action))\n",
    "#     print(len(item_freq_dict))\n",
    "#     freq_list = list(item_freq_dict.values())\n",
    "#     print(min(freq_list), max(freq_list))\n",
    "#     freq_threshold_list = [0, 20, 80, 150, 200, 250, 300, 350]\n",
    "    \n",
    "    ### set bucket for each item in a time period\n",
    "    \n",
    "    time_itemid_bucketid_dict = {}\n",
    "    time_bucketid_itemidlist_dict = {}\n",
    "    time_bucket_freq_dict = {}\n",
    "    \n",
    "    sorted_time_list = sorted(list(time_item_freq_dict.keys()))\n",
    "    for time in sorted_time_list:\n",
    "        print(\"==\"*10+str(time)+\"==\"*10)\n",
    "        if time not in time_itemid_bucketid_dict:\n",
    "            time_itemid_bucketid_dict[time] = {}\n",
    "        if time not in time_bucketid_itemidlist_dict:\n",
    "            time_bucketid_itemidlist_dict[time] = {}\n",
    "        \n",
    "        bucket_freq_dict_time = set_bucket4item_time_cluster(time_item_freq_dict[time], time_itemid_bucketid_dict[time], time_bucketid_itemidlist_dict[time])\n",
    "        time_bucket_freq_dict[time] = bucket_freq_dict_time\n",
    "                \n",
    "    return time_itemid_bucketid_dict, time_bucketid_itemidlist_dict, time_bucket_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bucket4item_time(item_freq_dict_time, itemid_bucketid_dict_time, bucketid_itemidlist_dict_time):\n",
    "    freq_threshold_list = [0, 20, 30, 40, 80, 120, 240, 400]\n",
    "    bucket_freq_dict_time = {}\n",
    "    for itemid in item_freq_dict_time:\n",
    "        i = item_freq_dict_time[itemid]\n",
    "        bucketid = 0\n",
    "        if i <= freq_threshold_list[1]:\n",
    "            bucketid = 1\n",
    "        elif i <= freq_threshold_list[2]:\n",
    "            bucketid = 2\n",
    "        elif i <= freq_threshold_list[3]:\n",
    "            bucketid = 3\n",
    "        elif i <= freq_threshold_list[4]:\n",
    "            bucketid = 4\n",
    "        elif i <= freq_threshold_list[5]:\n",
    "            bucketid = 5\n",
    "        elif i <= freq_threshold_list[6]:\n",
    "            bucketid = 6\n",
    "        elif i <= freq_threshold_list[7]:\n",
    "            bucketid = 7\n",
    "        else:\n",
    "            bucketid = 8\n",
    "            \n",
    "        itemid_bucketid_dict_time[itemid] = bucketid\n",
    "        if bucketid not in bucketid_itemidlist_dict_time:\n",
    "            bucketid_itemidlist_dict_time[bucketid] = []\n",
    "        bucketid_itemidlist_dict_time[bucketid].append(itemid)\n",
    "        \n",
    "    print(\"bucket\", len(bucketid_itemidlist_dict_time), bucketid_itemidlist_dict_time.keys())\n",
    "#     for bucketid in bucketid_itemidlist_dict:\n",
    "    for bucketid in range(1, len(bucketid_itemidlist_dict_time)+1):\n",
    "        itemid_list_bucket = bucketid_itemidlist_dict_time[bucketid]\n",
    "        freq_bucket = 0\n",
    "        for itemid in itemid_list_bucket:\n",
    "            freq_bucket += item_freq_dict_time[itemid]\n",
    "        bucket_freq_dict_time[bucketid] = freq_bucket\n",
    "        print(\"bucket %d, freq: %d, item num: %d\"%(bucketid, freq_bucket, len(itemid_list_bucket)))\n",
    "#     print(\"++\"*20)\n",
    "    return bucket_freq_dict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def set_bucket4item_time_cluster(item_freq_dict_time, itemid_bucketid_dict_time, bucketid_itemidlist_dict_time):\n",
    "    num_clusters = 5\n",
    "    bucket_freq_dict_time = {}\n",
    "    popularities = np.array(list(item_freq_dict_time.values()))\n",
    "    if len(popularities) > 0:\n",
    "        clusters = KMeans(num_clusters)\n",
    "        labels = clusters.fit_predict(popularities.reshape(-1, 1))\n",
    "        centroids = clusters.cluster_centers_\n",
    "        sorted_centroids = sorted(centroids)\n",
    "        sorted_clusters = [] #Cluster number at position of magnitude, least to greatest\n",
    "        for num in range(num_clusters):\n",
    "            i = np.where(centroids == sorted_centroids[num]) # Gives original cluster number\n",
    "            sorted_clusters.append(i[0][0])\n",
    "        \n",
    "        print(sorted_clusters)\n",
    "        for i, label in enumerate(labels):\n",
    "            label = sorted_clusters.index(label)\n",
    "            itemid = list(item_freq_dict_time.keys())[i]\n",
    "            itemid_bucketid_dict_time[itemid] = label\n",
    "            if label not in bucketid_itemidlist_dict_time:\n",
    "                bucketid_itemidlist_dict_time[label] = []\n",
    "            bucketid_itemidlist_dict_time[label].append(itemid)\n",
    "            \n",
    "        print(\"bucket\", len(bucketid_itemidlist_dict_time), bucketid_itemidlist_dict_time.keys())\n",
    "        for bucketid in range(len(bucketid_itemidlist_dict_time)):\n",
    "            itemid_list_bucket = bucketid_itemidlist_dict_time[bucketid]\n",
    "            freq_bucket = 0\n",
    "            for itemid in itemid_list_bucket:\n",
    "                freq_bucket += item_freq_dict_time[itemid]\n",
    "            bucket_freq_dict_time[bucketid] = freq_bucket\n",
    "            print(\"bucket %d, freq: %d, item num: %d\"%(bucketid, freq_bucket, len(itemid_list_bucket)))\n",
    "        \n",
    "        for cluster in range(num_clusters):\n",
    "            print(\"cluster %d, center %f\"%(cluster, centroids[cluster]))\n",
    "            \n",
    "    return bucket_freq_dict_time\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_item_freq_dict = {}\n",
    "# get_time_item_freq(train_data_loader, time_item_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_time_item_freq(test_data_loader, time_item_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 3, 2, 4, 5, 6, 0])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_item_freq_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================0====================\n",
      "====================1====================\n",
      "[1, 2, 0, 3, 4]\n",
      "bucket 5 dict_keys([1, 0, 2, 3, 4])\n",
      "bucket 0, freq: 31977, item num: 24301\n",
      "bucket 1, freq: 23047, item num: 6404\n",
      "bucket 2, freq: 13515, item num: 1815\n",
      "bucket 3, freq: 4796, item num: 323\n",
      "bucket 4, freq: 1233, item num: 39\n",
      "cluster 0, center 7.446281\n",
      "cluster 1, center 1.315872\n",
      "cluster 2, center 3.598844\n",
      "cluster 3, center 14.848297\n",
      "cluster 4, center 31.615385\n",
      "====================2====================\n",
      "[1, 0, 3, 2, 4]\n",
      "bucket 5 dict_keys([1, 2, 0, 3, 4])\n",
      "bucket 0, freq: 57627, item num: 33708\n",
      "bucket 1, freq: 60165, item num: 11005\n",
      "bucket 2, freq: 27182, item num: 2083\n",
      "bucket 3, freq: 10496, item num: 365\n",
      "bucket 4, freq: 2182, item num: 31\n",
      "cluster 0, center 5.467060\n",
      "cluster 1, center 1.709594\n",
      "cluster 2, center 28.756164\n",
      "cluster 3, center 13.049448\n",
      "cluster 4, center 70.387097\n",
      "====================3====================\n",
      "[0, 2, 1, 4, 3]\n",
      "bucket 5 dict_keys([1, 0, 2, 4, 3])\n",
      "bucket 0, freq: 81481, item num: 37839\n",
      "bucket 1, freq: 95910, item num: 13564\n",
      "bucket 2, freq: 47225, item num: 2691\n",
      "bucket 3, freq: 18290, item num: 463\n",
      "bucket 4, freq: 4734, item num: 49\n",
      "cluster 0, center 2.153360\n",
      "cluster 1, center 17.549238\n",
      "cluster 2, center 7.070923\n",
      "cluster 3, center 96.612245\n",
      "cluster 4, center 39.503240\n",
      "====================4====================\n",
      "[0, 3, 1, 4, 2]\n",
      "bucket 5 dict_keys([0, 3, 1, 2, 4])\n",
      "bucket 0, freq: 176551, item num: 40054\n",
      "bucket 1, freq: 238029, item num: 15826\n",
      "bucket 2, freq: 134919, item num: 3531\n",
      "bucket 3, freq: 51130, item num: 558\n",
      "bucket 4, freq: 11117, item num: 47\n",
      "cluster 0, center 4.407824\n",
      "cluster 1, center 38.209856\n",
      "cluster 2, center 236.531915\n",
      "cluster 3, center 15.040377\n",
      "cluster 4, center 91.630824\n",
      "====================5====================\n",
      "[0, 3, 1, 2, 4]\n",
      "bucket 5 dict_keys([2, 1, 3, 0, 4])\n",
      "bucket 0, freq: 341918, item num: 42170\n",
      "bucket 1, freq: 418808, item num: 14570\n",
      "bucket 2, freq: 246284, item num: 3465\n",
      "bucket 3, freq: 93844, item num: 556\n",
      "bucket 4, freq: 20401, item num: 47\n",
      "cluster 0, center 8.108086\n",
      "cluster 1, center 71.077633\n",
      "cluster 2, center 168.784173\n",
      "cluster 3, center 28.744544\n",
      "cluster 4, center 434.063830\n",
      "====================6====================\n",
      "[2, 0, 1, 3, 4]\n",
      "bucket 5 dict_keys([1, 3, 2, 0, 4])\n",
      "bucket 0, freq: 692177, item num: 42968\n",
      "bucket 1, freq: 857050, item num: 14595\n",
      "bucket 2, freq: 471091, item num: 3186\n",
      "bucket 3, freq: 174430, item num: 501\n",
      "bucket 4, freq: 39530, item num: 45\n",
      "cluster 0, center 58.722165\n",
      "cluster 1, center 147.862837\n",
      "cluster 2, center 16.109128\n",
      "cluster 3, center 348.163673\n",
      "cluster 4, center 878.444444\n"
     ]
    }
   ],
   "source": [
    "time_itemid_bucketid_dict, time_bucketid_itemidlist_dict, time_bucket_freq_dict = set_bucket4item(time_item_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================time 1====================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-77626b3bbc83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket_freq_dict_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbase_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_bucket_freq_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mdiff_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbase_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bucket:%d, base freq:%d, freq:%d, diff:%d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "sorted_time_list = sorted(list(time_bucket_freq_dict.keys()))\n",
    "base_bucket_freq_dict = time_bucket_freq_dict[sorted_time_list[0]]\n",
    "for time in sorted_time_list[1:]:\n",
    "    bucket_freq_dict_time = time_bucket_freq_dict[time]\n",
    "    sorted_bucket_list = sorted(list(bucket_freq_dict_time.keys()))\n",
    "    print(\"==\"*10+\"time \"+str(time)+\"==\"*10)\n",
    "    for bucket in sorted_bucket_list:\n",
    "        freq = bucket_freq_dict_time[bucket]\n",
    "        \n",
    "        base_freq = base_bucket_freq_dict[bucket]\n",
    "        diff_freq = freq-base_freq\n",
    "        print(\"bucket:%d, base freq:%d, freq:%d, diff:%d\"%(bucket, base_freq, freq, diff_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get bucket for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_bucket4item_train(item_freq_dict, itemid_bucketid_dict, bucketid_itemidlist_dict):\n",
    "#     freq_threshold_list = [0, 20, 30, 40, 80, 120, 240, 400]\n",
    "#     for itemid in item_freq_dict:\n",
    "#         i = item_freq_dict[itemid]\n",
    "#         bucketid = 0\n",
    "#         if i <= freq_threshold_list[1]:\n",
    "#             bucketid = 1\n",
    "#         elif i <= freq_threshold_list[2]:\n",
    "#             bucketid = 2\n",
    "#         elif i <= freq_threshold_list[3]:\n",
    "#             bucketid = 3\n",
    "#         elif i <= freq_threshold_list[4]:\n",
    "#             bucketid = 4\n",
    "#         elif i <= freq_threshold_list[5]:\n",
    "#             bucketid = 5\n",
    "#         elif i <= freq_threshold_list[6]:\n",
    "#             bucketid = 6\n",
    "#         elif i <= freq_threshold_list[7]:\n",
    "#             bucketid = 7\n",
    "#         else:\n",
    "#             bucketid = 8\n",
    "            \n",
    "#         itemid_bucketid_dict[itemid] = bucketid\n",
    "#         if bucketid not in bucketid_itemidlist_dict:\n",
    "#             bucketid_itemidlist_dict[bucketid] = []\n",
    "#         bucketid_itemidlist_dict[bucketid].append(itemid)\n",
    "        \n",
    "#     print(\"bucket\", len(bucketid_itemidlist_dict), bucketid_itemidlist_dict.keys())\n",
    "# #     for bucketid in bucketid_itemidlist_dict:\n",
    "#     for bucketid in range(1, len(bucketid_itemidlist_dict)+1):\n",
    "#         itemid_list_bucket = bucketid_itemidlist_dict[bucketid]\n",
    "#         freq_bucket = 0\n",
    "#         for itemid in itemid_list_bucket:\n",
    "#             freq_bucket += item_freq_dict[itemid]\n",
    "#         print(\"bucket %d, freq: %d, item num: %d\"%(bucketid, freq_bucket, len(itemid_list_bucket)))\n",
    "#     print(\"===\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_bucket4item_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-c23845b9b3fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_itemid_bucketid_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_bucketid_itemidlist_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mset_bucket4item_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_item_freq_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_itemid_bucketid_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bucketid_itemidlist_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'set_bucket4item_train' is not defined"
     ]
    }
   ],
   "source": [
    "train_itemid_bucketid_dict = {}\n",
    "train_bucketid_itemidlist_dict = {}\n",
    "set_bucket4item_train(train_item_freq_dict, train_itemid_bucketid_dict, train_bucketid_itemidlist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling\n"
     ]
    }
   ],
   "source": [
    "time_item_recall_dict, time_item_mrr_dict = bias_eval(valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_item_freq_dict = {}\n",
    "# time_bucket_recall_dict = {}\n",
    "# time_bucket_mrr_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bucket_recall_dict = {}\n",
    "time_bucket_mrr_dict = {}\n",
    "for time in time_item_recall_dict:\n",
    "    item_freq_dict_time = time_item_freq_dict[time]\n",
    "    item_recall_dict_time = time_item_recall_dict[time]\n",
    "    item_mrr_dict_time = time_item_mrr_dict[time]\n",
    "    itemid_bucketid_dict_time = time_itemid_bucketid_dict[time]\n",
    "    bucketid_itemidlist_dict_time = time_bucketid_itemidlist_dict[time]\n",
    "    \n",
    "    if time not in time_bucket_recall_dict:\n",
    "        time_bucket_recall_dict[time] = {}\n",
    "        time_bucket_mrr_dict[time] = {}\n",
    "    \n",
    "    for item in item_recall_dict_time:\n",
    "        bucketid = itemid_bucketid_dict_time[item]\n",
    "        item_recall = np.mean(item_recall_dict_time[item])\n",
    "\n",
    "        if bucketid not in time_bucket_recall_dict[time]:\n",
    "            time_bucket_recall_dict[time][bucketid] = []\n",
    "            time_bucket_mrr_dict[time][bucketid] = []\n",
    "            \n",
    "        time_bucket_recall_dict[time][bucketid].append(item_recall)\n",
    "\n",
    "        item_mrr = np.mean(item_mrr_dict_time[item])\n",
    "        time_bucket_mrr_dict[time][bucketid].append(item_mrr)\n",
    "\n",
    "    for bucket in time_bucket_recall_dict[time]:\n",
    "        recall_list = time_bucket_recall_dict[time][bucket]\n",
    "        mean_recall = np.mean(recall_list)\n",
    "        time_bucket_recall_dict[time][bucket] = mean_recall\n",
    "\n",
    "    for bucket in time_bucket_mrr_dict[time]:\n",
    "        mrr_list = time_bucket_mrr_dict[time][bucket]\n",
    "        mean_mrr = np.mean(mrr_list)\n",
    "        time_bucket_mrr_dict[time][bucket] = mean_mrr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************recall********************\n",
      "------------------------------time 1------------------------------\n",
      "1:0.2296, 2:0.2870, 3:0.2831, 4:0.2971, \n",
      "------------------------------time 2------------------------------\n",
      "1:0.2084, 2:0.2461, 3:0.2697, 4:0.2625, \n",
      "------------------------------time 3------------------------------\n",
      "1:0.1765, 2:0.2187, 3:0.2507, 4:0.2956, \n",
      "------------------------------time 4------------------------------\n",
      "1:0.1596, 2:0.1951, 3:0.2402, 4:0.2487, \n",
      "------------------------------time 5------------------------------\n",
      "1:0.1377, 2:0.1678, 3:0.2099, 4:0.2916, \n",
      "------------------------------time 6------------------------------\n",
      "1:0.1332, 2:0.1813, 3:0.1936, 4:0.2442, \n",
      "********************mrr********************\n",
      "------------------------------time 1------------------------------\n",
      "1:0.1052, 2:0.1379, 3:0.1328, 4:0.1328, \n",
      "------------------------------time 2------------------------------\n",
      "1:0.0950, 2:0.1050, 3:0.1059, 4:0.0948, \n",
      "------------------------------time 3------------------------------\n",
      "1:0.0782, 2:0.0853, 3:0.0919, 4:0.1171, \n",
      "------------------------------time 4------------------------------\n",
      "1:0.0677, 2:0.0748, 3:0.0895, 4:0.0889, \n",
      "------------------------------time 5------------------------------\n",
      "1:0.0527, 2:0.0564, 3:0.0691, 4:0.1152, \n",
      "------------------------------time 6------------------------------\n",
      "1:0.0509, 2:0.0582, 3:0.0728, 4:0.0946, \n"
     ]
    }
   ],
   "source": [
    "print(\"**\"*10+\"recall\"+\"**\"*10)\n",
    "for time in range(1, 7):\n",
    "    if time not in time_bucket_recall_dict:\n",
    "        continue\n",
    "    \n",
    "    print(\"--\"*15+\"time \"+str(time)+\"--\"*15)\n",
    "    for k in range(1, 9):\n",
    "        if k not in time_bucket_recall_dict[time]:\n",
    "            continue\n",
    "        recall = time_bucket_recall_dict[time][k]\n",
    "        print(\"%d:%.4f\"%(k, recall), end=\", \")\n",
    "    print()\n",
    "\n",
    "print(\"**\"*10+\"mrr\"+\"**\"*10)\n",
    "for time in range(1, 7):\n",
    "    if time not in time_bucket_mrr_dict:\n",
    "        continue\n",
    "    print(\"--\"*15+\"time \"+str(time)+\"--\"*15)\n",
    "    for k in range(1, 9):\n",
    "        if k not in time_bucket_mrr_dict[time]:\n",
    "            continue\n",
    "        mrr = time_bucket_mrr_dict[time][k]\n",
    "        print(\"%d:%.4f\"%(k, mrr), end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
