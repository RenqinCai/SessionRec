{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LogUniformSampler' from 'log_uniform' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c8207bda1587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SequentialRecommendation/SessionRec/biasRNN/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../../PyTorch_GBW_LM/log_uniform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlog_uniform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogUniformSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LogUniformSampler' from 'log_uniform' (unknown location)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "use time to cut sequences\n",
    "command \n",
    "python main_time.py --data_folder ../Data/xing/ --train_data train_item.pickle --valid_data test_item.pickle --test_data test_item.pickle --data_name xing --embedding_dim 300 --hidden_size 300 --lr 0.005\n",
    "\"\"\"\n",
    "import argparse\n",
    "import torch\n",
    "# import lib\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from loss import *\n",
    "from network import *\n",
    "from optimizer import *\n",
    "from trainer import *\n",
    "from torch.utils import data\n",
    "import pickle\n",
    "import sys\n",
    "from dataset_time import *\n",
    "# from data_time import *\n",
    "from logger import *\n",
    "import collections\n",
    "\n",
    "from sampledSoftmax import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/sr3hd/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--hidden_size', default=50, type=int)\n",
    "parser.add_argument('--num_layers', default=1, type=int)\n",
    "parser.add_argument('--batch_size', default=100, type=int)\n",
    "parser.add_argument('--dropout_input', default=0, type=float)\n",
    "parser.add_argument('--dropout_hidden', default=.2, type=float)\n",
    "\n",
    "# parse the optimizer arguments\n",
    "parser.add_argument('--optimizer_type', default='Adagrad', type=str)\n",
    "parser.add_argument('--final_act', default='tanh', type=str)\n",
    "parser.add_argument('--lr', default=.05, type=float)\n",
    "parser.add_argument('--weight_decay', default=0.0, type=float)\n",
    "parser.add_argument('--momentum', default=0.1, type=float)\n",
    "parser.add_argument('--eps', default=1e-6, type=float)\n",
    "\n",
    "parser.add_argument(\"-seed\", type=int, default=7,\n",
    "                     help=\"Seed for random initialization\")\n",
    "parser.add_argument(\"-sigma\", type=float, default=None,\n",
    "                     help=\"init weight -1: range [-sigma, sigma], -2: range [0, sigma]\")\n",
    "parser.add_argument(\"--embedding_dim\", type=int, default=-1,\n",
    "                     help=\"using embedding\")\n",
    "# parse the loss type\n",
    "parser.add_argument('--loss_type', default='TOP1', type=str)\n",
    "# parser.add_argument('--loss_type', default='BPR', type=str)\n",
    "parser.add_argument('--topk', default=5, type=int)\n",
    "# etc\n",
    "parser.add_argument('--bptt', default=1, type=int)\n",
    "parser.add_argument('--test_observed', default=5, type=int)\n",
    "parser.add_argument('--window_size', default=30, type=int)\n",
    "parser.add_argument('--warm_start', default=5, type=int)\n",
    "\n",
    "parser.add_argument('--n_epochs', default=20, type=int)\n",
    "parser.add_argument('--time_sort', default=False, type=bool)\n",
    "parser.add_argument('--save_dir', default='models', type=str)\n",
    "parser.add_argument('--data_folder', default='../Data/movielen/1m/', type=str)\n",
    "parser.add_argument('--data_action', default='item.pickle', type=str)\n",
    "parser.add_argument('--data_cate', default='cate.pickle', type=str)\n",
    "parser.add_argument('--data_time', default='time.pickle', type=str)\n",
    "parser.add_argument(\"--is_eval\", action='store_true')\n",
    "parser.add_argument('--load_model', default=None,  type=str)\n",
    "parser.add_argument('--checkpoint_dir', type=str, default='checkpoint')\n",
    "parser.add_argument('--data_name', default=None, type=str)\n",
    "parser.add_argument('--shared_embedding', default=None, type=int)\n",
    "parser.add_argument('--patience', default=1000)\n",
    "parser.add_argument('--negative_num', default=1000, type=int)\n",
    "parser.add_argument('--valid_start_time', default=0, type=int)\n",
    "parser.add_argument('--test_start_time', default=0, type=int)\n",
    "parser.add_argument('--model_name', default=\"samplePaddingSessionRNN\", type=str)\n",
    "\n",
    "# Get the arguments\n",
    "args = parser.parse_args([])\n",
    "args.cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=0 python eval_main_time.py --data_folder ../Data/tmall/100k_unknown_cate/ \n",
    "# --data_action item_time.pickle --data_cate cate_time.pickle --data_time time_time.pickle \n",
    "# --data_name taobao --embedding_dim 300 --hidden_size 300 --lr 0.001 --window_size 20 \n",
    "# --test_observed 5 --n_epochs 100 --shared_embedding 1 --batch_size 300 \n",
    "# --optimizer_type Adam --loss_type 'XE' --valid_start_time 1512172800 --test_start_time 1512259200 \n",
    "# --negative_num 10000 --topk 20 --checkpoint_dir \"../log/samplePaddingSessionRNN/checkpoint/01022149\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_folder = \"../Data/tmall/100k_unknown_cate/\"\n",
    "args.data_action = \"item_time.pickle\"\n",
    "args.data_cate = \"cate_time.pickle\"\n",
    "args.data_time = \"time_time.pickle\"\n",
    "args.data_name = \"taobao\"\n",
    "args.embedding_dim = 256\n",
    "args.hidden_size = 256\n",
    "args.lr = 0.001\n",
    "args.window_size = 20\n",
    "args.test_observed = 5\n",
    "args.n_epochs = 100\n",
    "args.shared_embedding = 1\n",
    "args.batch_size = 256\n",
    "args.optimizer_type = \"Adam\"\n",
    "args.loss_type = \"XE\"\n",
    "args.valid_start_time = 1512172800\n",
    "args.test_start_time = 1512259200\n",
    "args.negative_num = 10000\n",
    "args.topk = 20\n",
    "args.checkpoint_dir = \"../log/samplePaddingSessionRNN/checkpoint/01031151\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(7)\n",
    "random.seed(args.seed)\n",
    "\n",
    "if args.cuda:\n",
    "    print(\"gpu\")\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "else:\n",
    "    print(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1512172800: 12/02/2017 @ 12:00am (UTC)\n",
    "### 1512187200: 12/02/2017 @ 4:00am (UTC)\n",
    "## 1512201600: 12/02/2017 @ 8:00am (UTC)\n",
    "### 1512216000: 12/02/2017 @ 12:00pm (UTC)\n",
    "### 1512230400: 12/02/2017 @ 4:00pm (UTC)\n",
    "### 1512244800: 12/02/2017 @ 8:00pm (UTC)\n",
    "# 1512259200: 12/03/2017 @ 12:00am (UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_checkpoint_dir(log):\n",
    "    print(\"PARAMETER\" + \"-\"*10)\n",
    "    now = datetime.datetime.now()\n",
    "    S = '{:02d}{:02d}{:02d}{:02d}'.format(now.month, now.day, now.hour, now.minute)\n",
    "    checkpoint_dir = \"../log/\"+args.model_name+\"/\"+args.checkpoint_dir\n",
    "    args.checkpoint_dir = checkpoint_dir\n",
    "    save_dir = os.path.join(args.checkpoint_dir, S)\n",
    "\n",
    "    if not os.path.exists(\"../log\"):\n",
    "        os.mkdir(\"../log\")\n",
    "    \n",
    "    if not os.path.exists(\"../log/\"+args.model_name):\n",
    "        os.mkdir(\"../log/\"+args.model_name)\n",
    "\n",
    "    if not os.path.exists(args.checkpoint_dir):\n",
    "        os.mkdir(args.checkpoint_dir)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    args.checkpoint_dir = save_dir\n",
    "    \n",
    "    with open(os.path.join(args.checkpoint_dir, 'parameter.txt'), 'w') as f:\n",
    "        for attr, value in sorted(args.__dict__.items()):\n",
    "            msg = \"{}={}\".format(attr.upper(), value)\n",
    "            log.addOutput2IO(msg)\n",
    "            f.write(\"{}={}\\n\".format(attr.upper(), value))\n",
    "\n",
    "    msg = \"---------\" + \"-\"*10\n",
    "    log.addOutput2IO(msg)\n",
    "\n",
    "def load_args(model_path):\n",
    "    model_file = os.path.join(model_path, \"model_best.pt\")\n",
    "    print(\"args file load\", model_file)\n",
    "    check_point = torch.load(model_file)\n",
    "    args = check_point['args']\n",
    "\n",
    "def load_model(network, model_path):\n",
    "    print(\"reload model\")\n",
    "    model_file = os.path.join(model_path, \"model_best.pt\")\n",
    "    print(\"model file\", model_file)\n",
    "    check_point = torch.load(model_file)\n",
    "\n",
    "    network.load_state_dict(check_point['model'])\n",
    "\n",
    "def count_parameters(model):\n",
    "    parameter_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"parameter_num\", parameter_num) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args file load ../log/samplePaddingSessionRNN/checkpoint/01031151/model_best.pt\n",
      "device cuda\n",
      "**********train load**********\n",
      "action seq num 51275\n",
      "time seq num 51275\n",
      "loading item map\n",
      "loading item map\n",
      "observed_threshold 5 20\n",
      "loading data\n",
      "valid_start_time 1512172800\n",
      "test start time 1512259200\n",
      "seq num for training 2738883\n",
      "seq num of actions for training 2738883\n",
      "seq num for testing 430797\n",
      "seq num of actions for testing 430797\n",
      "load data duration  0:00:08.790854\n",
      "++++++++++valid load++++++++++\n",
      "item num 68008\n",
      "seq num 2738883\n",
      "batch size 256\n",
      "batch_num 10698\n",
      "seq num 430797\n",
      "batch size 256\n",
      "batch_num 1682\n",
      "seq num 430797\n",
      "batch size 256\n",
      "batch_num 1682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zf15/rc7ne/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reload model\n",
      "model file ../log/samplePaddingSessionRNN/checkpoint/01031151/model_best.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = args.checkpoint_dir\n",
    "load_args(model_path)\n",
    "\n",
    "BPTT = args.bptt\n",
    "\n",
    "device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "print(\"device\", device)\n",
    "\n",
    "if args.embedding_dim == -1:\n",
    "    raise AssertionError()\n",
    "\n",
    "data_name = args.data_name\n",
    "\n",
    "print(\"*\"*10+\"train load\"+\"*\"*10)\n",
    "\n",
    "observed_threshold = args.test_observed\n",
    "\n",
    "data_action = args.data_folder+args.data_action\n",
    "data_cate = args.data_folder+args.data_cate\n",
    "data_time = args.data_folder+args.data_time\n",
    "\n",
    "valid_start_time = args.valid_start_time\n",
    "test_start_time = args.test_start_time\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "data_obj = MYDATA(data_action, data_cate, data_time, valid_start_time, test_start_time, observed_threshold, args.window_size)\n",
    "et = datetime.datetime.now()\n",
    "print(\"load data duration \", et-st)\n",
    "\n",
    "train_data = data_obj.train_dataset\n",
    "valid_data = data_obj.test_dataset\n",
    "test_data = data_obj.test_dataset\n",
    "\n",
    "print(\"+\"*10+\"valid load\"+\"+\"*10)\n",
    "\n",
    "input_size = data_obj.items()\n",
    "output_size = input_size\n",
    "\n",
    "negative_num = args.negative_num\n",
    "\n",
    "train_data_loader = MYDATALOADER(train_data, args.batch_size)\n",
    "valid_data_loader = MYDATALOADER(valid_data, args.batch_size)\n",
    "test_data_loader = MYDATALOADER(valid_data, args.batch_size)\n",
    "\n",
    "ss = SampledSoftmax(output_size, negative_num, args.embedding_dim, None)\n",
    "\n",
    "network = NETWORK(input_size, ss, args, device)\n",
    "load_model(network, model_path)\n",
    "\n",
    "### eval\n",
    "loss_function = LossFunction(device, loss_type=args.loss_type)\n",
    "\n",
    "topk = args.topk\n",
    "eval = Evaluation(None, network, loss_function, device, topk, args.warm_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get time id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1512172800: 12/02/2017 @ 12:00am (UTC)\n",
    "### 1512187200: 12/02/2017 @ 4:00am (UTC)\n",
    "## 1512201600: 12/02/2017 @ 8:00am (UTC)\n",
    "### 1512216000: 12/02/2017 @ 12:00pm (UTC)\n",
    "### 1512230400: 12/02/2017 @ 4:00pm (UTC)\n",
    "### 1512244800: 12/02/2017 @ 8:00pm (UTC)\n",
    "# 1512259200: 12/03/2017 @ 12:00am (UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeid(time):\n",
    "    time_threshold_list = [1512172800, 1512187200, 1512201600, 1512216000, 1512230400, 1512244800, 1512259200]\n",
    "#     print(\"time threshold num\", len(time_threshold_list))\n",
    "    \n",
    "    timeid = 0\n",
    "    \n",
    "    if time <= time_threshold_list[1]:\n",
    "        timeid = 1\n",
    "    elif time <= time_threshold_list[2]:\n",
    "        timeid = 2\n",
    "    elif time <= time_threshold_list[3]:\n",
    "        timeid = 3\n",
    "    elif time <= time_threshold_list[4]:\n",
    "        timeid = 4\n",
    "    elif time <= time_threshold_list[5]:\n",
    "        timeid = 5\n",
    "    else:\n",
    "        timeid = 6\n",
    "        \n",
    "    return timeid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_item_freq_dict = {}\n",
    "# time_bucket_recall_dict = {}\n",
    "# time_bucket_mrr_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_eval(eval_data):\n",
    "    network.eval()\n",
    "\n",
    "    losses = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    weights = []\n",
    "\n",
    "    dataloader = eval_data\n",
    "    topk = args.topk\n",
    "    \n",
    "    ### time: item: [recall]\n",
    "    time_item_recall_dict = {}\n",
    "    time_item_mrr_dict = {}\n",
    "\n",
    "    \n",
    "### t_y_batch: time of y action\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_test_num = []\n",
    "\n",
    "        for x_short_action_batch, mask_short_action_batch, pad_x_short_actionNum_batch, \\\n",
    "        y_action_batch, y_action_idx_batch, t_y_batch in dataloader:\n",
    "            \n",
    "            x_short_action_batch = x_short_action_batch.to(device)\n",
    "            mask_short_action_batch = mask_short_action_batch.to(device)\n",
    "            y_action_batch = y_action_batch.to(device)\n",
    "\n",
    "            # warm_start_mask = (y_action_idx_batch>=self.warm_start)\n",
    "\n",
    "            output_batch = network(x_short_action_batch, mask_short_action_batch, pad_x_short_actionNum_batch)\n",
    "\n",
    "            sampled_logit_batch, sampled_target_batch = network.m_ss(output_batch, y_action_batch, \\\n",
    "                                                        None, None, None, None, None, None, \"full\")\n",
    "\n",
    "            loss_batch = loss_function(sampled_logit_batch, sampled_target_batch)\n",
    "            losses.append(loss_batch.item())\n",
    "\n",
    "            _, preds = torch.topk(sampled_logit_batch, topk, -1)\n",
    "            preds = preds.cpu()\n",
    "            targets = sampled_target_batch.cpu()\n",
    "\n",
    "            expand_targets = targets.view(-1, 1).expand_as(preds)\n",
    "            hits = (preds == expand_targets)\n",
    "\n",
    "            for i, hit in enumerate(hits):\n",
    "                target_i = targets[i]\n",
    "                itemid_i = target_i.item()\n",
    "                time_i = t_y_batch[i].item()\n",
    "                timeid = get_timeid(time_i)\n",
    "                \n",
    "                if timeid not in time_item_recall_dict:\n",
    "                    time_item_recall_dict[timeid] = {}\n",
    "                    time_item_mrr_dict[timeid] = {}\n",
    "                if itemid_i not in time_item_recall_dict[timeid]:\n",
    "                    time_item_recall_dict[timeid][itemid_i] = []\n",
    "                    time_item_mrr_dict[timeid][itemid_i] = []\n",
    "                \n",
    "                rank = hit.nonzero()\n",
    "                \n",
    "                if len(rank) == 1:\n",
    "                    time_item_recall_dict[timeid][itemid_i].append(1.0)\n",
    "                    rank = rank[0]+1.0\n",
    "                    rank = torch.reciprocal(rank.float())\n",
    "                    time_item_mrr_dict[timeid][itemid_i].append(rank.item())\n",
    "                else:\n",
    "                    time_item_recall_dict[timeid][itemid_i].append(0.0)\n",
    "                    time_item_mrr_dict[timeid][itemid_i].append(0.0)\n",
    "                \n",
    "            total_test_num.append(y_action_batch.view(-1).size(0))\n",
    "            \n",
    "    return time_item_recall_dict, time_item_mrr_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_item_freq(eval_data, time_item_freq_dict):\n",
    "    network.eval()\n",
    "\n",
    "    losses = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    weights = []\n",
    "\n",
    "    dataloader = eval_data\n",
    "    topk = args.topk\n",
    "    \n",
    "    ### time: item: freq\n",
    "    \n",
    "    ### t_action_batch: time of y action\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_test_num = []\n",
    "\n",
    "        for x_short_action_batch, mask_short_action_batch, pad_x_short_actionNum_batch, \\\n",
    "        y_action_batch, y_action_idx_batch, t_y_batch in dataloader:\n",
    "            batch_size = y_action_batch.size(0)\n",
    "            for seq_index in range(batch_size):\n",
    "                y_i = y_action_batch[seq_index]\n",
    "                t_i = t_y_batch[seq_index]\n",
    "                item_i = y_i.item()\n",
    "                time_id = get_timeid(t_i.item())\n",
    "                \n",
    "                if time_id not in time_item_freq_dict:\n",
    "                    time_item_freq_dict[time_id] = {}\n",
    "                if item_i not in time_item_freq_dict[time_id]:\n",
    "                    time_item_freq_dict[time_id][item_i] = 0.0\n",
    "                time_item_freq_dict[time_id][item_i] += 1.0\n",
    "#     return time_item_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_freq(eval_data, item_freq_dict):\n",
    "    network.eval()\n",
    "\n",
    "    losses = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    weights = []\n",
    "\n",
    "    dataloader = eval_data\n",
    "    topk = args.topk\n",
    "    \n",
    "    ### time: item: freq\n",
    "    \n",
    "    ### t_action_batch: time of y action\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_test_num = []\n",
    "\n",
    "        for x_short_action_batch, mask_short_action_batch, pad_x_short_actionNum_batch, \\\n",
    "        y_action_batch, y_action_idx_batch, t_y_batch in dataloader:\n",
    "            batch_size = y_action_batch.size(0)\n",
    "            for seq_index in range(batch_size):\n",
    "                y_i = y_action_batch[seq_index]\n",
    "                t_i = t_y_batch[seq_index]\n",
    "                item_i = y_i.item()\n",
    "                \n",
    "                if item_i not in item_freq_dict:\n",
    "                    item_freq_dict[item_i] = 0.0\n",
    "                item_freq_dict[item_i] += 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_freq(time_item_freq_dict, item_freq_dict):\n",
    "    for time in time_item_freq_dict:\n",
    "        item_freq_dict_time = time_item_freq_dict[time]\n",
    "        \n",
    "        for item in item_freq_dict:\n",
    "            if item not in item_freq_dict_time:\n",
    "                item_freq_dict_time[item] = 0.0\n",
    "            item_freq_dict_time[item] += item_freq_dict[item]\n",
    "    \n",
    "    time_item_freq_dict[0] = {}\n",
    "    for item in item_freq_dict:\n",
    "        time_item_freq_dict[0][item] = item_freq_dict[item]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling\n"
     ]
    }
   ],
   "source": [
    "train_item_freq_dict = {}\n",
    "get_item_freq(train_data_loader, train_item_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling\n"
     ]
    }
   ],
   "source": [
    "time_item_freq_dict = {}\n",
    "get_time_item_freq(test_data_loader, time_item_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_train_freq(time_item_freq_dict, train_item_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bucket4item(time_item_freq_dict):\n",
    "#     item_freq_dict = dict(Counter(data.m_y_action))\n",
    "#     print(len(item_freq_dict))\n",
    "#     freq_list = list(item_freq_dict.values())\n",
    "#     print(min(freq_list), max(freq_list))\n",
    "#     freq_threshold_list = [0, 20, 80, 150, 200, 250, 300, 350]\n",
    "    \n",
    "    ### set bucket for each item in a time period\n",
    "    \n",
    "    time_itemid_bucketid_dict = {}\n",
    "    time_bucketid_itemidlist_dict = {}\n",
    "    time_bucket_freq_dict = {}\n",
    "    \n",
    "    sorted_time_list = sorted(list(time_item_freq_dict.keys()))\n",
    "    for time in sorted_time_list:\n",
    "        print(\"==\"*10+str(time)+\"==\"*10)\n",
    "        if time not in time_itemid_bucketid_dict:\n",
    "            time_itemid_bucketid_dict[time] = {}\n",
    "        if time not in time_bucketid_itemidlist_dict:\n",
    "            time_bucketid_itemidlist_dict[time] = {}\n",
    "        \n",
    "        bucket_freq_dict_time = set_bucket4item_time(time_item_freq_dict[time], time_itemid_bucketid_dict[time], time_bucketid_itemidlist_dict[time])\n",
    "        \n",
    "        time_bucket_freq_dict[time] = bucket_freq_dict_time\n",
    "                \n",
    "    return time_itemid_bucketid_dict, time_bucketid_itemidlist_dict, time_bucket_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bucket4item_time(item_freq_dict_time, itemid_bucketid_dict_time, bucketid_itemidlist_dict_time):\n",
    "    freq_threshold_list = [0, 20, 30, 40, 80, 120, 240, 400]\n",
    "    bucket_freq_dict_time = {}\n",
    "    for itemid in item_freq_dict_time:\n",
    "        i = item_freq_dict_time[itemid]\n",
    "        bucketid = 0\n",
    "        if i <= freq_threshold_list[1]:\n",
    "            bucketid = 1\n",
    "        elif i <= freq_threshold_list[2]:\n",
    "            bucketid = 2\n",
    "        elif i <= freq_threshold_list[3]:\n",
    "            bucketid = 3\n",
    "        elif i <= freq_threshold_list[4]:\n",
    "            bucketid = 4\n",
    "        elif i <= freq_threshold_list[5]:\n",
    "            bucketid = 5\n",
    "        elif i <= freq_threshold_list[6]:\n",
    "            bucketid = 6\n",
    "        elif i <= freq_threshold_list[7]:\n",
    "            bucketid = 7\n",
    "        else:\n",
    "            bucketid = 8\n",
    "            \n",
    "        itemid_bucketid_dict_time[itemid] = bucketid\n",
    "        if bucketid not in bucketid_itemidlist_dict_time:\n",
    "            bucketid_itemidlist_dict_time[bucketid] = []\n",
    "        bucketid_itemidlist_dict_time[bucketid].append(itemid)\n",
    "        \n",
    "    print(\"bucket\", len(bucketid_itemidlist_dict_time), bucketid_itemidlist_dict_time.keys())\n",
    "#     for bucketid in bucketid_itemidlist_dict:\n",
    "    for bucketid in range(1, len(bucketid_itemidlist_dict_time)+1):\n",
    "        itemid_list_bucket = bucketid_itemidlist_dict_time[bucketid]\n",
    "        freq_bucket = 0\n",
    "        for itemid in itemid_list_bucket:\n",
    "            freq_bucket += item_freq_dict_time[itemid]\n",
    "        bucket_freq_dict_time[bucketid] = freq_bucket\n",
    "        print(\"bucket %d, freq: %d, item num: %d\"%(bucketid, freq_bucket, len(itemid_list_bucket)))\n",
    "#     print(\"++\"*20)\n",
    "    return bucket_freq_dict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_item_freq_dict = {}\n",
    "# get_time_item_freq(train_data_loader, time_item_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_time_item_freq(test_data_loader, time_item_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2, 4, 1, 3, 6, 5, 0])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_item_freq_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================0====================\n",
      "bucket 8 dict_keys([3, 4, 5, 1, 8, 2, 6, 7])\n",
      "bucket 1, freq: 347360, item num: 21237\n",
      "bucket 2, freq: 476991, item num: 19241\n",
      "bucket 3, freq: 323545, item num: 9261\n",
      "bucket 4, freq: 676329, item num: 12280\n",
      "bucket 5, freq: 311280, item num: 3198\n",
      "bucket 6, freq: 343391, item num: 2116\n",
      "bucket 7, freq: 143984, item num: 482\n",
      "bucket 8, freq: 115808, item num: 191\n",
      "====================1====================\n",
      "bucket 8 dict_keys([6, 3, 2, 1, 4, 5, 7, 8])\n",
      "bucket 1, freq: 322607, item num: 19642\n",
      "bucket 2, freq: 490533, item num: 19775\n",
      "bucket 3, freq: 334497, item num: 9569\n",
      "bucket 4, freq: 704058, item num: 12764\n",
      "bucket 5, freq: 321419, item num: 3297\n",
      "bucket 6, freq: 362327, item num: 2240\n",
      "bucket 7, freq: 152573, item num: 512\n",
      "bucket 8, freq: 125242, item num: 207\n",
      "====================2====================\n",
      "bucket 8 dict_keys([3, 7, 4, 1, 6, 2, 5, 8])\n",
      "bucket 1, freq: 320582, item num: 19484\n",
      "bucket 2, freq: 490863, item num: 19792\n",
      "bucket 3, freq: 337623, item num: 9656\n",
      "bucket 4, freq: 707299, item num: 12804\n",
      "bucket 5, freq: 322943, item num: 3309\n",
      "bucket 6, freq: 362850, item num: 2239\n",
      "bucket 7, freq: 152609, item num: 512\n",
      "bucket 8, freq: 127003, item num: 210\n",
      "====================3====================\n",
      "bucket 8 dict_keys([6, 4, 2, 1, 5, 3, 8, 7])\n",
      "bucket 1, freq: 318047, item num: 19344\n",
      "bucket 2, freq: 492722, item num: 19865\n",
      "bucket 3, freq: 338301, item num: 9670\n",
      "bucket 4, freq: 707349, item num: 12812\n",
      "bucket 5, freq: 324577, item num: 3328\n",
      "bucket 6, freq: 367087, item num: 2264\n",
      "bucket 7, freq: 152895, item num: 512\n",
      "bucket 8, freq: 127698, item num: 211\n",
      "====================4====================\n",
      "bucket 8 dict_keys([4, 3, 6, 1, 5, 2, 8, 7])\n",
      "bucket 1, freq: 306530, item num: 18599\n",
      "bucket 2, freq: 498733, item num: 20062\n",
      "bucket 3, freq: 341442, item num: 9753\n",
      "bucket 4, freq: 724344, item num: 13115\n",
      "bucket 5, freq: 329820, item num: 3385\n",
      "bucket 6, freq: 377684, item num: 2338\n",
      "bucket 7, freq: 159741, item num: 536\n",
      "bucket 8, freq: 132280, item num: 218\n",
      "====================5====================\n",
      "bucket 8 dict_keys([6, 8, 4, 2, 1, 5, 3, 7])\n",
      "bucket 1, freq: 338230, item num: 20624\n",
      "bucket 2, freq: 481208, item num: 19407\n",
      "bucket 3, freq: 328693, item num: 9407\n",
      "bucket 4, freq: 687905, item num: 12480\n",
      "bucket 5, freq: 316230, item num: 3245\n",
      "bucket 6, freq: 349520, item num: 2153\n",
      "bucket 7, freq: 146768, item num: 492\n",
      "bucket 8, freq: 119783, item num: 198\n",
      "====================6====================\n",
      "bucket 8 dict_keys([7, 2, 4, 3, 6, 5, 8, 1])\n",
      "bucket 1, freq: 339844, item num: 20756\n",
      "bucket 2, freq: 480722, item num: 19394\n",
      "bucket 3, freq: 328488, item num: 9401\n",
      "bucket 4, freq: 682993, item num: 12391\n",
      "bucket 5, freq: 314894, item num: 3233\n",
      "bucket 6, freq: 347691, item num: 2143\n",
      "bucket 7, freq: 147898, item num: 495\n",
      "bucket 8, freq: 117575, item num: 193\n"
     ]
    }
   ],
   "source": [
    "time_itemid_bucketid_dict, time_bucketid_itemidlist_dict, time_bucket_freq_dict = set_bucket4item(time_item_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================time 1====================\n",
      "bucket:1, base freq:347360, freq:322607, diff:-24753\n",
      "bucket:2, base freq:476991, freq:490533, diff:13542\n",
      "bucket:3, base freq:323545, freq:334497, diff:10952\n",
      "bucket:4, base freq:676329, freq:704058, diff:27729\n",
      "bucket:5, base freq:311280, freq:321419, diff:10139\n",
      "bucket:6, base freq:343391, freq:362327, diff:18936\n",
      "bucket:7, base freq:143984, freq:152573, diff:8589\n",
      "bucket:8, base freq:115808, freq:125242, diff:9434\n",
      "====================time 2====================\n",
      "bucket:1, base freq:347360, freq:320582, diff:-26778\n",
      "bucket:2, base freq:476991, freq:490863, diff:13872\n",
      "bucket:3, base freq:323545, freq:337623, diff:14078\n",
      "bucket:4, base freq:676329, freq:707299, diff:30970\n",
      "bucket:5, base freq:311280, freq:322943, diff:11663\n",
      "bucket:6, base freq:343391, freq:362850, diff:19459\n",
      "bucket:7, base freq:143984, freq:152609, diff:8625\n",
      "bucket:8, base freq:115808, freq:127003, diff:11195\n",
      "====================time 3====================\n",
      "bucket:1, base freq:347360, freq:318047, diff:-29313\n",
      "bucket:2, base freq:476991, freq:492722, diff:15731\n",
      "bucket:3, base freq:323545, freq:338301, diff:14756\n",
      "bucket:4, base freq:676329, freq:707349, diff:31020\n",
      "bucket:5, base freq:311280, freq:324577, diff:13297\n",
      "bucket:6, base freq:343391, freq:367087, diff:23696\n",
      "bucket:7, base freq:143984, freq:152895, diff:8911\n",
      "bucket:8, base freq:115808, freq:127698, diff:11890\n",
      "====================time 4====================\n",
      "bucket:1, base freq:347360, freq:306530, diff:-40830\n",
      "bucket:2, base freq:476991, freq:498733, diff:21742\n",
      "bucket:3, base freq:323545, freq:341442, diff:17897\n",
      "bucket:4, base freq:676329, freq:724344, diff:48015\n",
      "bucket:5, base freq:311280, freq:329820, diff:18540\n",
      "bucket:6, base freq:343391, freq:377684, diff:34293\n",
      "bucket:7, base freq:143984, freq:159741, diff:15757\n",
      "bucket:8, base freq:115808, freq:132280, diff:16472\n",
      "====================time 5====================\n",
      "bucket:1, base freq:347360, freq:338230, diff:-9130\n",
      "bucket:2, base freq:476991, freq:481208, diff:4217\n",
      "bucket:3, base freq:323545, freq:328693, diff:5148\n",
      "bucket:4, base freq:676329, freq:687905, diff:11576\n",
      "bucket:5, base freq:311280, freq:316230, diff:4950\n",
      "bucket:6, base freq:343391, freq:349520, diff:6129\n",
      "bucket:7, base freq:143984, freq:146768, diff:2784\n",
      "bucket:8, base freq:115808, freq:119783, diff:3975\n",
      "====================time 6====================\n",
      "bucket:1, base freq:347360, freq:339844, diff:-7516\n",
      "bucket:2, base freq:476991, freq:480722, diff:3731\n",
      "bucket:3, base freq:323545, freq:328488, diff:4943\n",
      "bucket:4, base freq:676329, freq:682993, diff:6664\n",
      "bucket:5, base freq:311280, freq:314894, diff:3614\n",
      "bucket:6, base freq:343391, freq:347691, diff:4300\n",
      "bucket:7, base freq:143984, freq:147898, diff:3914\n",
      "bucket:8, base freq:115808, freq:117575, diff:1767\n"
     ]
    }
   ],
   "source": [
    "sorted_time_list = sorted(list(time_bucket_freq_dict.keys()))\n",
    "base_bucket_freq_dict = time_bucket_freq_dict[sorted_time_list[0]]\n",
    "for time in sorted_time_list[1:]:\n",
    "    bucket_freq_dict_time = time_bucket_freq_dict[time]\n",
    "    sorted_bucket_list = sorted(list(bucket_freq_dict_time.keys()))\n",
    "    print(\"==\"*10+\"time \"+str(time)+\"==\"*10)\n",
    "    for bucket in sorted_bucket_list:\n",
    "        freq = bucket_freq_dict_time[bucket]\n",
    "        \n",
    "        base_freq = base_bucket_freq_dict[bucket]\n",
    "        diff_freq = freq-base_freq\n",
    "        print(\"bucket:%d, base freq:%d, freq:%d, diff:%d\"%(bucket, base_freq, freq, diff_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get bucket for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_bucket4item_train(item_freq_dict, itemid_bucketid_dict, bucketid_itemidlist_dict):\n",
    "#     freq_threshold_list = [0, 20, 30, 40, 80, 120, 240, 400]\n",
    "#     for itemid in item_freq_dict:\n",
    "#         i = item_freq_dict[itemid]\n",
    "#         bucketid = 0\n",
    "#         if i <= freq_threshold_list[1]:\n",
    "#             bucketid = 1\n",
    "#         elif i <= freq_threshold_list[2]:\n",
    "#             bucketid = 2\n",
    "#         elif i <= freq_threshold_list[3]:\n",
    "#             bucketid = 3\n",
    "#         elif i <= freq_threshold_list[4]:\n",
    "#             bucketid = 4\n",
    "#         elif i <= freq_threshold_list[5]:\n",
    "#             bucketid = 5\n",
    "#         elif i <= freq_threshold_list[6]:\n",
    "#             bucketid = 6\n",
    "#         elif i <= freq_threshold_list[7]:\n",
    "#             bucketid = 7\n",
    "#         else:\n",
    "#             bucketid = 8\n",
    "            \n",
    "#         itemid_bucketid_dict[itemid] = bucketid\n",
    "#         if bucketid not in bucketid_itemidlist_dict:\n",
    "#             bucketid_itemidlist_dict[bucketid] = []\n",
    "#         bucketid_itemidlist_dict[bucketid].append(itemid)\n",
    "        \n",
    "#     print(\"bucket\", len(bucketid_itemidlist_dict), bucketid_itemidlist_dict.keys())\n",
    "# #     for bucketid in bucketid_itemidlist_dict:\n",
    "#     for bucketid in range(1, len(bucketid_itemidlist_dict)+1):\n",
    "#         itemid_list_bucket = bucketid_itemidlist_dict[bucketid]\n",
    "#         freq_bucket = 0\n",
    "#         for itemid in itemid_list_bucket:\n",
    "#             freq_bucket += item_freq_dict[itemid]\n",
    "#         print(\"bucket %d, freq: %d, item num: %d\"%(bucketid, freq_bucket, len(itemid_list_bucket)))\n",
    "#     print(\"===\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket 8 dict_keys([1, 4, 5, 2, 6, 3, 7, 8])\n",
      "bucket 1, freq: 347360, item num: 21237\n",
      "bucket 2, freq: 476991, item num: 19241\n",
      "bucket 3, freq: 323545, item num: 9261\n",
      "bucket 4, freq: 676329, item num: 12280\n",
      "bucket 5, freq: 311280, item num: 3198\n",
      "bucket 6, freq: 343391, item num: 2116\n",
      "bucket 7, freq: 143984, item num: 482\n",
      "bucket 8, freq: 115808, item num: 191\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "train_itemid_bucketid_dict = {}\n",
    "train_bucketid_itemidlist_dict = {}\n",
    "set_bucket4item_train(train_item_freq_dict, train_itemid_bucketid_dict, train_bucketid_itemidlist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zf15/rc7ne/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729006826/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n"
     ]
    }
   ],
   "source": [
    "time_item_recall_dict, time_item_mrr_dict = bias_eval(valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_item_freq_dict = {}\n",
    "# time_bucket_recall_dict = {}\n",
    "# time_bucket_mrr_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bucket_recall_dict = {}\n",
    "time_bucket_mrr_dict = {}\n",
    "for time in time_item_recall_dict:\n",
    "    item_freq_dict_time = time_item_freq_dict[time]\n",
    "    item_recall_dict_time = time_item_recall_dict[time]\n",
    "    item_mrr_dict_time = time_item_mrr_dict[time]\n",
    "    itemid_bucketid_dict_time = time_itemid_bucketid_dict[time]\n",
    "    bucketid_itemidlist_dict_time = time_bucketid_itemidlist_dict[time]\n",
    "    \n",
    "    if time not in time_bucket_recall_dict:\n",
    "        time_bucket_recall_dict[time] = {}\n",
    "        time_bucket_mrr_dict[time] = {}\n",
    "    \n",
    "    for item in item_recall_dict_time:\n",
    "        bucketid = itemid_bucketid_dict_time[item]\n",
    "        item_recall = np.mean(item_recall_dict_time[item])\n",
    "\n",
    "        if bucketid not in time_bucket_recall_dict[time]:\n",
    "            time_bucket_recall_dict[time][bucketid] = []\n",
    "            time_bucket_mrr_dict[time][bucketid] = []\n",
    "            \n",
    "        time_bucket_recall_dict[time][bucketid].append(item_recall)\n",
    "\n",
    "        item_mrr = np.mean(item_mrr_dict_time[item])\n",
    "        time_bucket_mrr_dict[time][bucketid].append(item_mrr)\n",
    "\n",
    "    for bucket in time_bucket_recall_dict[time]:\n",
    "        recall_list = time_bucket_recall_dict[time][bucket]\n",
    "        mean_recall = np.mean(recall_list)\n",
    "        time_bucket_recall_dict[time][bucket] = mean_recall\n",
    "\n",
    "    for bucket in time_bucket_mrr_dict[time]:\n",
    "        mrr_list = time_bucket_mrr_dict[time][bucket]\n",
    "        mean_mrr = np.mean(mrr_list)\n",
    "        time_bucket_mrr_dict[time][bucket] = mean_mrr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************recall********************\n",
      "------------------------------time 1------------------------------\n",
      "1:0.1183, 2:0.1404, 3:0.1565, 4:0.1676, 5:0.1890, 6:0.2161, 7:0.2188, 8:0.2653, \n",
      "------------------------------time 2------------------------------\n",
      "1:0.1045, 2:0.1339, 3:0.1547, 4:0.1681, 5:0.1931, 6:0.2119, 7:0.2478, 8:0.3033, \n",
      "------------------------------time 3------------------------------\n",
      "1:0.1008, 2:0.1259, 3:0.1429, 4:0.1538, 5:0.1764, 6:0.2062, 7:0.2539, 8:0.2947, \n",
      "------------------------------time 4------------------------------\n",
      "1:0.0885, 2:0.1188, 3:0.1290, 4:0.1561, 5:0.1799, 6:0.2105, 7:0.2458, 8:0.2943, \n",
      "------------------------------time 5------------------------------\n",
      "1:0.0882, 2:0.1086, 3:0.1262, 4:0.1366, 5:0.1479, 6:0.1867, 7:0.2289, 8:0.2926, \n",
      "------------------------------time 6------------------------------\n",
      "1:0.0939, 2:0.1097, 3:0.1219, 4:0.1290, 5:0.1440, 6:0.1904, 7:0.2102, 8:0.2728, \n",
      "********************mrr********************\n",
      "------------------------------time 1------------------------------\n",
      "1:0.0573, 2:0.0646, 3:0.0682, 4:0.0677, 5:0.0666, 6:0.0740, 7:0.0725, 8:0.0894, \n",
      "------------------------------time 2------------------------------\n",
      "1:0.0498, 2:0.0655, 3:0.0705, 4:0.0688, 5:0.0684, 6:0.0719, 7:0.0838, 8:0.1038, \n",
      "------------------------------time 3------------------------------\n",
      "1:0.0497, 2:0.0610, 3:0.0634, 4:0.0610, 5:0.0600, 6:0.0689, 7:0.0868, 8:0.1062, \n",
      "------------------------------time 4------------------------------\n",
      "1:0.0420, 2:0.0546, 3:0.0551, 4:0.0621, 5:0.0619, 6:0.0703, 7:0.0858, 8:0.1073, \n",
      "------------------------------time 5------------------------------\n",
      "1:0.0380, 2:0.0478, 3:0.0514, 4:0.0503, 5:0.0494, 6:0.0567, 7:0.0696, 8:0.1086, \n",
      "------------------------------time 6------------------------------\n",
      "1:0.0437, 2:0.0483, 3:0.0512, 4:0.0459, 5:0.0452, 6:0.0577, 7:0.0689, 8:0.0951, \n"
     ]
    }
   ],
   "source": [
    "print(\"**\"*10+\"recall\"+\"**\"*10)\n",
    "for time in range(1, 7):\n",
    "    if time not in time_bucket_recall_dict:\n",
    "        continue\n",
    "    \n",
    "    print(\"--\"*15+\"time \"+str(time)+\"--\"*15)\n",
    "    for k in range(1, 9):\n",
    "        if k not in time_bucket_recall_dict[time]:\n",
    "            continue\n",
    "        recall = time_bucket_recall_dict[time][k]\n",
    "        print(\"%d:%.4f\"%(k, recall), end=\", \")\n",
    "    print()\n",
    "\n",
    "print(\"**\"*10+\"mrr\"+\"**\"*10)\n",
    "for time in range(1, 7):\n",
    "    if time not in time_bucket_mrr_dict:\n",
    "        continue\n",
    "    print(\"--\"*15+\"time \"+str(time)+\"--\"*15)\n",
    "    for k in range(1, 9):\n",
    "        if k not in time_bucket_mrr_dict[time]:\n",
    "            continue\n",
    "        mrr = time_bucket_mrr_dict[time][k]\n",
    "        print(\"%d:%.4f\"%(k, mrr), end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_train_item_freq_dict = {k:v for k, v in sorted(train_item_freq_dict.items(), key=lambda x: x[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_itemid_list = list(sorted_train_item_freq_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_train_item_freq_dict[591]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------eval train--------------------\n",
      "shuffling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/zf15/rc7ne/Project/SessionRecommendation/biasRNN/metric.py:19: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729006826/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  hits = hits.nonzero()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 7.2635, recall: 0.3523, mrr: 0.1444\n"
     ]
    }
   ],
   "source": [
    "print(\"--\"*10+\"eval train\"+\"--\"*10)\n",
    "mean_loss, mean_recall, mean_mrr = eval.eval(train_data_loader, \"train\")\n",
    "msg = \"train loss: {:.4f}, recall: {:.4f}, mrr: {:.4f}\".format(mean_loss, mean_recall, mean_mrr)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bucketid(itemid, itemid_bucketid_dict):\n",
    "    \n",
    "#     return itemid_bucketid_dict[itemid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias_eval(train_data_loader, train_item_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling\n"
     ]
    }
   ],
   "source": [
    "item_recall_dict, item_mrr_dict = bias_eval(train_data_loader, train_itemid_bucketid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_bucket_recall_dict = {k:v for k, v in sorted(bucket_recall_dict.items(), key=lambda x: x[0], reverse=True)}\n",
    "sorted_bucket_mrr_dict = {k:v for k, v in sorted(bucket_mrr_dict.items(), key=lambda x: x[0], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8:0.2876, 7:0.2662, 6:0.2387, 5:0.2377, 4:0.2323, 3:0.2063, 2:0.1842, 1:0.1418, \n"
     ]
    }
   ],
   "source": [
    "for k in sorted_bucket_recall_dict:\n",
    "#     print(k)\n",
    "    recall = sorted_bucket_recall_dict[k]\n",
    "    print(\"%d:%.4f\"%(k, recall), end=\", \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:0.3125, 2:0.3152, 3:0.3273, 4:0.3542, 5:0.3554, 6:0.3485, 7:0.3432, 8:0.3814, \n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    print(\"%d:%.4f\"%(i, bucket_recall_dict[i]), end=\", \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:0.1363, 2:0.1108, 3:0.1134, 4:0.1110, 5:0.1166, 6:0.1209, 7:0.1363, 8:0.1324, \n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    print(\"%d:%.4f\"%(i, bucket_mrr_dict[i]), end=\", \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall\n",
      "1 recall [154503.0, 399485.0]\n",
      "1:0.3868, 2 recall [98387.0, 273779.0]\n",
      "2:0.3594, 3 recall [108131.0, 304190.0]\n",
      "3:0.3555, 4 recall [142787.0, 405289.0]\n",
      "4:0.3523, 5 recall [101641.0, 291182.0]\n",
      "5:0.3491, 6 recall [147265.0, 422700.0]\n",
      "6:0.3484, 7 recall [106963.0, 311422.0]\n",
      "7:0.3435, 8 recall [105438.0, 330641.0]\n",
      "8:0.3189, \n",
      "mrr\n",
      "1 mrr [54172.382816109806, 399485.0]\n",
      "1:0.1356, 2 mrr [33972.77482056245, 273779.0]\n",
      "2:0.1241, 3 mrr [39045.844110224396, 304190.0]\n",
      "3:0.1284, 4 mrr [55967.14206555113, 405289.0]\n",
      "4:0.1381, 5 mrr [42985.19073393941, 291182.0]\n",
      "5:0.1476, 6 mrr [66511.61650883779, 422700.0]\n",
      "6:0.1573, 7 mrr [51561.4920447655, 311422.0]\n",
      "7:0.1656, 8 mrr [51641.94107887894, 330641.0]\n",
      "8:0.1562, \n"
     ]
    }
   ],
   "source": [
    "print(\"recall\")\n",
    "train_recall_list = []\n",
    "bucketid_list = []\n",
    "for k in bucket_recall_dict:\n",
    "    \n",
    "    recall = bucket_recall_dict[k]\n",
    "    print(k, \"recall\", recall)\n",
    "    if recall[1] == 0:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    recall = recall[0]/recall[1]\n",
    "    print(\"%.d:%.4f\"%(k, recall), end=\", \")\n",
    "    \n",
    "    bucketid_list.append(k)\n",
    "    train_recall_list.append(recall)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"mrr\")\n",
    "train_mrr_list = []\n",
    "for k in bucket_mrr_dict:\n",
    "    mrr = bucket_mrr_dict[k]\n",
    "    print(k, \"mrr\", mrr)\n",
    "    if mrr[1] == 0:\n",
    "        continue\n",
    "    \n",
    "    mrr = mrr[0]/mrr[1]\n",
    "    print(\"%.d:%.4f\"%(k, mrr), end=\", \")\n",
    "    \n",
    "    train_mrr_list.append(mrr)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_recall_list = [0.1870, 0.2044, 0.2074, 0.2160, 0.2199, 0.2239, 0.2347, 0.2844]\n",
    "# train_mrr_list = [0.1052, 0.1161, 0.1160, 0.1196, 0.1183, 0.1148, 0.1142, 0.1153]\n",
    "# freq_list = [i for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
